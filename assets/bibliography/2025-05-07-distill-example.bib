@article{gregor2015draw,
  title={DRAW: A recurrent neural network for image generation},
  author={Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
  journal={arXiv preprint, arXiv:1502.04623},
  year={2015},
  url={https://arxiv.org/pdf/1502.04623.pdf}
}

@article{fort2019deep,
  title={Deep ensembles: A loss landscape perspective},
  author={Fort, Stanislav and Hu, Huiyi and Lakshminarayanan, Balaji},
  journal={arXiv preprint arXiv:1912.02757},
  year={2019},
  url = {https://arxiv.org/abs/1912.02757}
}

@article{hansen1990neural,
  title={Neural network ensembles},
  author={Hansen, Lars Kai and Salamon, Peter},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={12},
  number={10},
  pages={993--1001},
  year={1990},
  publisher={IEEE},
  url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=58871}
}

@article{klabunde2023similarity,
  title={Similarity of neural network models: A survey of functional and representational measures},
  author={Klabunde, Max and Schumacher, Tobias and Strohmaier, Markus and Lemmerich, Florian},
  journal={arXiv preprint arXiv:2305.06329},
  year={2023}, 
  url = {https://arxiv.org/pdf/2305.06329}
}

@article{li2018visualizing,
  title={Visualizing the loss landscape of neural nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018},
  url= {https://proceedings.neurips.cc/paper_files/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada},
  url = {https://www.cs.toronto.edu/~kriz/cifar.html}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020},
  url = {https://arxiv.org/pdf/2010.11929}
}

@article{van2008visualizing,
  title={Visualizing data using t-SNE.},
  author={Van der Maaten, Laurens and Hinton, Geoffrey},
  journal={Journal of machine learning research},
  volume={9},
  number={11},
  url ={https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf},
  year={2008}
}

@inproceedings{chundawat2023can,
author = {Chundawat, Vikram S and Tarun, Ayush K and Mandal, Murari and Kankanhalli, Mohan},
title = {Can bad teaching induce forgetting? unlearning in deep networks using an incompetent teacher},
year = {2023},
isbn = {978-1-57735-880-0},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v37i6.25879},
doi = {10.1609/aaai.v37i6.25879},
abstract = {Machine unlearning has become an important area of research due to an increasing need for machine learning (ML) applications to comply with the emerging data privacy regulations. It facilitates the provision for removal of certain set or class of data from an already trained ML model without requiring retraining from scratch. Recently, several efforts have been put in to make unlearning to be effective and efficient. We propose a novel machine unlearning method by exploring the utility of competent and incompetent teachers in a student-teacher framework to induce forgetfulness. The knowledge from the competent and incompetent teachers is selectively transferred to the student to obtain a model that doesn't contain any information about the forget data. We experimentally show that this method generalizes well, is fast and effective. Furthermore, we introduce the zero retrain forgetting (ZRF) metric to evaluate any unlearning method. Unlike the existing unlearning metrics, the ZRF score does not depend on the availability of the expensive retrained model. This makes it useful for analysis of the unlearned model after deployment as well. We present results of experiments conducted for random subset forgetting and class forgetting on various deep networks and across different application domains.},
booktitle = {Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {810},
numpages = {8},
series = {AAAI'23/IAAI'23/EAAI'23}
}

@article{lin1991divergence,
  title={Divergence measures based on the Shannon entropy},
  author={Lin, Jianhua},
  journal={IEEE Transactions on Information theory},
  volume={37},
  number={1},
  pages={145--151},
  year={1991},
  publisher={IEEE},
  url = {https://ieeexplore.ieee.org/document/61115}
}

@inproceedings{
mason-williams2024neural,
title={{NEURAL} {NETWORK} {COMPRESSION}: {THE} {FUNCTIONAL} {PERSPECTIVE}},
author={Israel Mason-Williams},
booktitle={5th Workshop on practical ML for limited/low resource settings},
year={2024},
url={https://openreview.net/forum?id=Q7GXKjmCSB}
}

@inproceedings{
mason-williams2024what,
title={What Makes a Good Prune? Maximal Unstructured Pruning for Maximal Cosine Similarity},
author={Gabryel Mason-Williams and Fredrik Dahlqvist},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=jsvvPVVzwf}
}

@inproceedings{
mason-williams2024knowledge,
title={Knowledge Distillation: The Functional Perspective},
author={Mason-Williams, Israel  and  Mason-Williams,Gabryel and Sandler, Mark},
booktitle={NeurIPS 2024 Workshop on Scientific Methods for Understanding Deep Learning},
year={2024},
url={https://openreview.net/forum?id=Cgo73ZnAQc}
}
